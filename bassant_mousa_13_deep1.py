# -*- coding: utf-8 -*-
"""Bassant Mousa 13 DEEP1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SO63QxKLCX1ipnD14Ml4e4WnAEwl1dQp
"""

# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

data = pd.read_csv('/content/FuelConsumptionCo2.csv')
data.head()

data.info()

data = data.drop_duplicates()

num = data.select_dtypes(include='number')
num.head()

for col in num:
  print(col)
  sns.histplot(num[col])
  plt.show()
  print("="*12)

num.columns

log= ['FUELCONSUMPTION_CITY' ,'FUELCONSUMPTION_COMB',]
for col in log:
  data[col] = np.log(data[col])

for col in log:
  sns.histplot(data[col])
  plt.show()
  print("="*12)

for col in num:
  sns.boxplot(data[col])
  plt.show()
  print("="*12)

cat = data.select_dtypes(include='object')
cat.head()

for col in cat:
  print(col)
  print(cat[col].value_counts())
  print("="*12)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in cat:
  data[col] = le.fit_transform(data[col])

data

data.isna().sum()

data

from sklearn.model_selection import train_test_split
X = data.drop('CO2EMISSIONS', axis=1)
y = data['CO2EMISSIONS']

x_train , x_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=100)

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
for col in x_train.columns:
  if x_train[col].dtype in ['int64', 'float64']: # Only scale numerical columns
    x_train[col] = scaler.fit_transform(x_train[[col]])
    x_test[col] = scaler.transform(x_test[[col]])

# use deeplearning for regression to predict 'CO2EMISSIONS'

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

model = Sequential()
model.add(Dense(128, input_dim=x_train.shape[1], activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))

early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')

history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop])
print(model.summary())

# ACCURACY
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

import joblib
joblib.dump(model, 'model.pkl')